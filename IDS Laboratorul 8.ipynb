{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_machine = pd.read_csv(\"machine.data\", header = None)\n",
    "dataframe_housing = pd.read_csv(\"housing.data\", header = None, sep=r\"\\s+\")\n",
    "dataframe_communities = pd.read_csv(\"communities.data\", header = None)\n",
    "dataframe_wpbc = pd.read_csv(\"wpbc.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computer Hardware Data Set\"\"\"\n",
    "dataframe_machine = dataframe_machine.drop([0, 1, 9], axis = 1)\n",
    "dataframe_machine_X = dataframe_machine.values[:, :-1]\n",
    "dataframe_machine_Y = dataframe_machine.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Boston Housing\"\"\"\n",
    "dataframe_housing_X = dataframe_housing.values[:, :-1]\n",
    "dataframe_housing_Y = dataframe_housing.values[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wisconsing Breast Cancer\"\"\"\n",
    "dataframe_wpbc=dataframe_wpbc.drop([1], axis=1)\n",
    "dataframe_wpbc_X = dataframe_wpbc.values[:, :-1]\n",
    "dataframe_wpbc_Y = dataframe_wpbc.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Communities and Crime\"\"\"\n",
    "dataframe_communities = dataframe_communities.drop([0,1,2,3,4], axis=1)\n",
    "missing_index = dataframe_communities.columns[(dataframe_communities == '?').any()].tolist()\n",
    "dataframe_communities = dataframe_communities.drop(missing_index, axis=1)\n",
    "\n",
    "dataframe_communities_X = dataframe_communities.values[:, :-1]\n",
    "dataframe_communities_Y = dataframe_communities.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(search, x_train, x_test, y_train, y_test):\n",
    "    errors_list = []\n",
    "    #Train \n",
    "    errors_list.append(cross_val_score(search, x_train, y_train, cv=3, scoring='neg_mean_absolute_error') * -1)\n",
    "    errors_list.append(cross_val_score(search, x_train, y_train, cv=3, scoring='neg_mean_squared_error') * -1)\n",
    "    errors_list.append(cross_val_score(search, x_train, y_train, cv=3, scoring='neg_median_absolute_error') * -1)\n",
    "    #Test\n",
    "    errors_list.append(cross_val_score(search, x_test, y_test, cv=3, scoring='neg_mean_absolute_error') * -1)\n",
    "    errors_list.append(cross_val_score(search, x_test, y_test, cv=3, scoring='neg_mean_squared_error') * -1)\n",
    "    errors_list.append(cross_val_score(search, x_test, y_test, cv=3, scoring='neg_median_absolute_error') * -1)\n",
    "    return errors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Search(parameter, model, x_train, x_test, y_train, y_test):\n",
    "    randomized_search = RandomizedSearchCV(estimator = model, param_distributions = parameter, \n",
    "                                scoring='neg_mean_squared_error', cv=3, return_train_score=True, iid=False)\n",
    "    randomized_search.fit(x_train, y_train)\n",
    "    return ['Random Search'] + errors(randomized_search, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Search(parameter, model, x_train, x_test, y_train, y_test):\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameter, \n",
    "                          scoring='neg_mean_squared_error', cv=3, return_train_score=True, iid=False)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    return ['Grid Search'] + errors(grid_search, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"K cel mai apropiat vecin este un algoritm simplu care stochează toate cazurile disponibile și prezice ținta numerică \n",
    "pe baza unei măsuri de similaritate (de exemplu, funcțiile de distanță). KNN a fost utilizată în estimarea statistică și \n",
    "recunoașterea modelelor la începutul anilor 1970 ca o tehnică non-parametrică. O simplă implementare a regresiei KNN este \n",
    "de a calcula media țintei numerice a celor mai apropiați vecini K. O altă abordare folosește o medie ponderată inversă a \n",
    "celor mai apropiați vecini K. Regresia KNN utilizează aceleași funcții de distanță ca și clasificarea KNN. Formulele obisnuite\n",
    "pentru masurarea distantei sunt valabile numai pentru variabilele continue. În cazul variabilelor categorice, trebuie \n",
    "să utilizați formula pentru distanța Hamming, care reprezintă o măsură a numărului de cazuri în care simbolurile corespunzătoare\n",
    "sunt diferite în două șiruri de aceeași lungime.Alegerea valorii optime pentru K se face cel mai bine prin prima inspecție a \n",
    "datelor. În general, o valoare K mare este mai precisă, deoarece reduce zgomotul total; cu toate acestea, compromisul este că \n",
    "limitele distincte din spațiul caracteristicilor sunt neclare. Validarea încrucișată este o altă modalitate de determinare \n",
    "retrospectivă a unei valori K bune prin utilizarea unui set de date independent pentru a valida valoarea K. K optimal pentru \n",
    "majoritatea seturilor de date este de 10 sau mai mult. Asta produce rezultate mult mai bune decât 1-NN.\n",
    "KNN poate fi folosit atât pentru probleme de predicție de clasificare, cât și pentru regresie. Cu toate acestea, este utilizat\n",
    "mai mult în problemele de clasificare din industrie. Pentru a evalua orice tehnică, analizăm în general trei aspecte importante:\n",
    "Ușor de interpretat ieșirea, Timpul de calcul, Puterea predictivă\"\"\"\n",
    "\"\"\"\"https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\"\"\"\"\"\n",
    "\"\"\"https://www.saedsayad.com/k_nearest_neighbors_reg.htm\"\"\"\n",
    "\n",
    "\n",
    "def KNN_Regressor(x_train, y_train, x_test, y_test):\n",
    "    parameter = {'n_neighbors': list(range(1,12)), 'p': [1, 2, 3, 4.7]} \n",
    "    model = KNeighborsRegressor()\n",
    "    data.append(['KNN'] + Grid_Search(parameter, model, x_train, x_test, y_train, y_test))\n",
    "    data.append(['KNN'] + Random_Search(parameter, model, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asdfg](knn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SVM este o clasă foarte specifică a algoritmului, caracterizată prin utilizarea de kerneluri, absența minimelor locale, \n",
    "raritatea soluției și controlul capacității obținute prin acționarea pe margine sau pe numărul de vectori de sprijin etc.A fost \n",
    "inventat de Vladimir Vapnik și colegii săi și au fost introduși pentru prima oară la conferința cu lucrarea \"Teoria învățării \n",
    "computaționale (COLT) 1992\". Toate aceste caracteristici frumoase totuși au fost deja prezente în procesul de învățare mecanică \n",
    "începând cu anii 1960: utilizarea hiper-avioanelor de margini mari a kernelurilor, interpretarea geometrică a kernelurilor \n",
    "ca produse interioare într-un spațiu caracteristic. Tehnici similare de optimizare au fost folosite în recunoașterea modelului \n",
    "și tehnicile de spargare au fost discutate pe scară largă. Utilizarea variabilelor slabe pentru depășirea zgomotului în date \n",
    "și a separabilității a fost introdusă, de asemenea, în anii 1960. Cu toate acestea, până în anul 1992 toate aceste caracteristici\n",
    "au fost puse împreună pentru a forma clasificatorul de marjă maximă, mașina de suport de bază, și nu până în 1995, că a fost \n",
    "introdusă versiunea de marjă moale.\n",
    "SVM poate fi aplicată nu numai problemelor de clasificare, ci și în cazul regresiei. Totuși, acesta conține toate \n",
    "caracteristicile principale care caracterizează algoritmul de marjă maximă: o funcție neliniară este înclinată prin \n",
    "cartografiere liniară de învățare a mașinilor în spațiul caracteristic de dimensiuni mari induse de kernel. Capacitatea \n",
    "sistemului este controlată de parametrii care nu depind de dimensionalitatea spațiului caracteristic. În același mod ca și \n",
    "abordarea de clasificare există motivația de a căuta și de a optimiza limitele de generalizare date pentru regresie. Ei s-au \n",
    "bazat pe definirea funcției de pierdere care ignoră erorile care se află în anumite distanțe ale valorii reale. Acest tip de \n",
    "funcție este adesea numit - funcția de pierdere intensivă - epsilon. Figura de mai jos prezintă un exemplu de funcție de \n",
    "regresie liniară unidimensională cu banda intensivă - epsilon. Variabilele măsoară costul erorilor pe punctele de antrenament. \n",
    "Acestea sunt zero pentru toate punctele care se află în interiorul benzii. \"\"\"\n",
    "\"\"\"http://kernelsvm.tripod.com/\"\"\"\n",
    "\"\"\"https://www.saedsayad.com/support_vector_machine_reg.htm\"\"\"\n",
    "def SVM_Regressor(x_train, y_train, x_test, y_test):\n",
    "    parameter = {'C': list([0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 2.0, 3.0, 4.0, 5.0, 10.0])}\n",
    "    model = SVR()\n",
    "    data.append(['SVC'] + Grid_Search(parameter, model, x_train, x_test, y_train, y_test))\n",
    "    data.append(['SVC'] + Random_Search(parameter, model, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM regressor](svm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random forest sau pădurile decizionale aleatoare reprezinta o metodă de învățare a ansamblului pentru clasificare, regresie \n",
    "și alte sarcini care operează prin construirea unei multitudini de arbori de decizie la timpul de instruire și scoaterea clasei \n",
    "care este modul de clasificare (clasificare) sau predicție medie (regresie) a copacilor individuali. Pădurile decizionale \n",
    "aleatorii corectează obiceiul copiilor decizionali de a suprasolicita la setul lor de antrenament. Primul algoritm pentru \n",
    "pădurile decizionale aleatorii a fost creat de Tin Kam Ho utilizând metoda subspațiului aleatoriu, care, în formula lui Ho, \n",
    "este o modalitate de a pune în practică abordarea de clasificare propusă de Eugene Kleinberg. O extensie a algoritmului a fost \n",
    "dezvoltată de Leo Breiman și Adele Cutler, care au înregistrat \"Forest Random\" ca marcă comercială (din 2019, deținută de \n",
    "Minitab, Inc.) Extensia combină ideea \"bagging\" a lui Breiman și selecția aleatorie a caracteristicilor, introduse mai întâi \n",
    "de Ho și mai târziu independent de Amit și Geman pentru a construi o colecție de arbori de decizie cu variație controlată. \n",
    "Decision trees reprezintă o metodă populară pentru diferitele activități de învățare a mașinilor. Arborele de învățare \"vine \n",
    "cel mai aproape de a îndeplini cerințele pentru a servi ca o procedură de tip\" off-the-shelf \"pentru miniere de date\", spune \n",
    "Hastie și colab., \"Deoarece este invariantă sub scalare și diverse alte transformări ale valorilor caracteristice, la includerea\n",
    "caracteristicilor irelevante și produce modele inspectabile, însă acestea sunt rareori exacte\". \n",
    "\"O pădure aleatoare este o tehnică de ansamblu capabilă să efectueze atât sarcini de regresie, cât și sarcini de clasificare, \n",
    "folosind mai mulți copaci de decizie și o tehnică numită Bootstrap Aggregation, cunoscută în mod obișnuit ca \"bagging\". \n",
    "Ce poți să-ți pui bagajul? Bagging-ul, în metoda Random Forest, implică instruirea fiecărui arbore al deciziilor pe un eșantion \n",
    "de date diferit, unde prelevarea de probe se face cu înlocuirea. Ideea de bază din spatele acestei situații este combinarea \n",
    "mai multor arbori de decizie în determinarea producției finale, în loc să se bazeze pe arborii decizionali individuali. Dacă \n",
    "doriți să citiți mai multe despre Random Forest, am inclus câteva link-uri de referință care oferă explicații detaliate pe \n",
    "această temă.\"\"\"\n",
    "\"\"\"https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\"\"\"\n",
    "\"\"\"https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\"\"\"\n",
    "\n",
    "def Random_Forest_Regressor(x_train, y_train, x_test, y_test):\n",
    "    parameter = {'n_estimators': list(range(1,13))}\n",
    "    model = RandomForestRegressor()\n",
    "    data.append(['Forest'] + Grid_Search(parameter, model, x_train, x_test, y_train, y_test))\n",
    "    data.append(['Forest'] + Random_Search(parameter, model, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random_Forest](random-forest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decizia Arbore este un instrument de luare a deciziilor care utilizează o structură arborescentă ca un diagramă sau este un \n",
    "model de decizii și toate rezultatele posibile ale acestora, inclusiv rezultatele, costurile de intrare și utilitatea. \n",
    "Algoritmul algoritmului de decizie se încadrează în categoria algoritmilor de învățare supravegheați. Funcționează atât pentru \n",
    "variabilele de ieșire atât continue, cât și categorice. Ramurile / marginile reprezintă rezultatul nodului și nodurile au:\n",
    "condiții [noduri de decizie], rezultat [noduri finale], ramurile / marginile reprezintă adevărul / falsitatea afirmației și ia \n",
    "o decizie bazată pe cea din exemplul de mai jos care arată un arbore de decizie care evaluează cel mai mic dintre cele trei \n",
    "numere. Succesiunea regresiei arborelui observă caracteristicile unui obiect și pregătește un model din structura unui copac \n",
    "pentru a prezice date în viitor pentru a produce o ieșire continuă semnificativă. Ieșirea continuă înseamnă că rezultatul / \n",
    "rezultatul nu este discret, adică nu este reprezentat doar de un set discret, cunoscut de numere sau valori.\n",
    "În domeniul informaticii, învățarea arborelui decizional folosește un arbore de decizie (ca model predictiv) pentru a trece de \n",
    "la observațiile despre un element (reprezentat în ramuri) la concluziile privind valoarea țintă a elementului (reprezentată în \n",
    "frunze). Este una dintre abordările de modelare predictivă utilizate în statistici, în domeniul minelor de date și al învățării\n",
    "în mașină. Modelele de copaci în care variabila țintă poate lua un set discret de valori se numesc arbori de clasificare; în \n",
    "aceste structuri de copaci, frunzele reprezintă etichete de clasă, iar ramificațiile reprezintă conjuncții ale caracteristicilor\n",
    "care conduc la acele etichete de clasă. Arborii de decizie în care variabila țintă pot lua valori continue (de obicei numere \n",
    "reale) se numesc arbori de regresie. În analiza deciziei, un arbore de decizie poate fi folosit pentru a reprezenta în mod \n",
    "vizual și explicit deciziile și luarea deciziilor. În exploatarea datelor, un arbore de decizie descrie datele (dar arborele de \n",
    "clasificare rezultat poate fi o contribuție pentru luarea deciziilor). Această pagină se ocupă cu arborii de decizie în \n",
    "domeniul exploatării datelor.\"\"\"\n",
    "\"\"\"https://www.saedsayad.com/decision_tree_reg.htm\"\"\"\n",
    "\"\"\"https://en.wikipedia.org/wiki/Decision_tree_learning\"\"\"\n",
    "\n",
    "def Decision_Tree_Regressor(x_train, y_train, x_test, y_test):\n",
    "    parameter = {'random_state': list([1, 50, 100, 200, 300, 333, 500, 700, 1000, 2000, 3000, 10000])}\n",
    "    model = DecisionTreeRegressor()\n",
    "    data.append(['Tree'] + Grid_Search(parameter, model, x_train, x_test, y_train, y_test))\n",
    "    data.append(['Tree'] + Random_Search(parameter, model, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision_Tree](decision-tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"În teoria și statisticile de probabilitate, un proces Gaussian este un proces stochastic (o colecție de variabile aleatoare \n",
    "indexate de timp sau de spațiu), astfel încât fiecare colecție finită a acestor variabile aleatoare are o distribuție normală \n",
    "multivariată, adică fiecare combinație liniară finită a acestora este în mod normal distribuite. Distribuția unui proces \n",
    "Gaussian este distribuția comună a tuturor acelor variabile aleatoare (infinit de multe) și, ca atare, este o distribuție \n",
    "asupra funcțiilor cu un domeniu continuu, de ex. timp sau spațiu. Un algoritm de învățare a mașinilor care implică un proces \n",
    "Gaussian utilizează învățarea leneș și o măsură a asemănării dintre puncte (funcția kernelului) pentru a prezice valoarea \n",
    "pentru un punct nevăzut din datele de antrenament. Predicția nu este doar o estimare pentru acel punct, ci și o informație de \n",
    "incertitudine - este o distribuție Gaussiană unidimensională (care este distribuția marginală la acel punct). Pentru unele \n",
    "funcții de kernel, algebra matricială poate fi folosită pentru a calcula predicțiile folosind tehnica kriging. Când se \n",
    "utilizează un kernel parametrizat, software-ul de optimizare este de obicei folosit pentru a se potrivi unui model de proces \n",
    "Gaussian. Conceptul de procese Gauss este numit după Carl Friedrich Gauss pentru că se bazează pe noțiunea de distribuție \n",
    "Gaussiană (distribuție normală). Procesele Gaussian pot fi văzute ca o generalizare infinită-dimensională a distribuțiilor \n",
    "normale multivariate. Procesele gaussiene sunt utile în modelarea statistică, beneficiind de proprietăți moștenite din \n",
    "distribuția normală. Un proces Gaussian poate fi folosit ca distribuție de probabilitate anterioară față de inferența Bayesiană\n",
    "Având în vedere orice set de puncte N în domeniul dorit al funcțiilor dumneavoastră, luați un multivariat Gaussian a cărui \n",
    "parametru de matrice covarianță este matricea Gram a punctelor dvs. N cu un kernel dorit și o mostră din acel Gaussian. \n",
    "Inferența valorilor continue cu un proces Gaussian este cunoscută sub denumirea de regresie a procesului Gaussian, sau kriging; \n",
    "Regresia procesului Gaussian pentru variabilele țintă multiple este cunoscută sub denumirea de cocriging.Procesele Gauss sunt \n",
    "astfel utile ca un instrument puternic de interpolare neliniară multivariabilă. Regresia procesului Gauss poate fi extinsă în \n",
    "continuare pentru a aborda sarcinile de învățare atât în cadrele de învățare supravegheate (de exemplu, în clasificarea \n",
    "probabilistică), cât și în cele nesupravegheate\"\"\"\n",
    "\"\"\"www.gaussianprocess.org/gpml/chapters/RW2.pdf\"\"\"\n",
    "\"\"\"https://en.wikipedia.org/wiki/Gaussian_process\"\"\"\n",
    "\n",
    "\n",
    "def Gaussian_Process_Regressor(x_train, y_train, x_test, y_test):\n",
    "    parameter = {'alpha': list([0.0000000000001, 0.000000000001, 0.00000000001, 0.0000000001, 0.000000001, 0.00000001, 0.0000001, \n",
    "                                        0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0])}\n",
    "    model = GaussianProcessRegressor()\n",
    "    data.append(['GaussianProcessRegressor'] + Grid_Search(parameter, model, x_train, x_test, y_train, y_test))\n",
    "    data.append(['GaussianProcessRegressor'] + Random_Search(parameter, model, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gaussian_Process](gaussian.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: red' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run(x, y, name):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3)\n",
    "    \n",
    "    KNN_Regressor(x_train, y_train, x_test, y_test)\n",
    "    Random_Forest_Regressor(x_train, y_train, x_test, y_test)\n",
    "    Decision_Tree_Regressor(x_train, y_train, x_test, y_test)\n",
    "    Gaussian_Process_Regressor(x_train, y_train, x_test, y_test)\n",
    "    data_frame = pd.DataFrame(data, columns=['Model', 'Search', 'train_mean_absolute_error', 'train_mean_squared_error',\n",
    "                                        'train_median_absolute_error', 'test_mean_absolute_error', 'test_mean_squared_error', \n",
    "                                         'test_median_absolute_error'])\n",
    "    #data_frame = data_frame.style.apply(highlight_max, subset=['train_mean_absolute_error', 'train_mean_squared_error',\n",
    "                                        #'train_median_absolute_error', 'test_mean_absolute_error', 'test_mean_squared_error', \n",
    "                                        #'test_median_absolute_error']).apply(highlight_min, subset=['train_mean_absolute_error',\n",
    "                                        #'train_mean_squared_error', 'train_median_absolute_error', 'test_mean_absolute_error',\n",
    "                                        #'test_mean_squared_error', 'test_median_absolute_error'])\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run(dataframe_machine_X, dataframe_machine_Y, \"machine\").to_html('Machine.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run(dataframe_housing_X, dataframe_housing_Y, \"housing\").to_html('Housing.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run(dataframe_wpbc_X, dataframe_wpbc_Y, \"wpbc\").to_html('Wpbc.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run(dataframe_communities_X, dataframe_communities_Y, \"communities\").to_html(\"Comunities.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
